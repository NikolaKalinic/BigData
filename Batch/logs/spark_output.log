Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
:: loading settings :: url = jar:file:/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.postgresql#postgresql added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-df1cd383-1eff-46ba-8152-a69df105db9b;1.0
	confs: [default]
	found org.postgresql#postgresql;42.2.10 in central
downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.10/postgresql-42.2.10.jar ...
	[SUCCESSFUL ] org.postgresql#postgresql;42.2.10!postgresql.jar(bundle) (246ms)
:: resolution report :: resolve 1636ms :: artifacts dl 248ms
	:: modules in use:
	org.postgresql#postgresql;42.2.10 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   1   |   1   |   1   |   0   ||   1   |   1   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-df1cd383-1eff-46ba-8152-a69df105db9b
	confs: [default]
	1 artifacts copied, 0 already retrieved (905kB/7ms)
23/12/31 00:24:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
23/12/31 00:24:54 INFO SparkContext: Running Spark version 3.0.1
23/12/31 00:24:54 INFO ResourceUtils: ==============================================================
23/12/31 00:24:54 INFO ResourceUtils: Resources for spark.driver:

23/12/31 00:24:54 INFO ResourceUtils: ==============================================================
23/12/31 00:24:54 INFO SparkContext: Submitted application: batch-preprocessing
23/12/31 00:24:54 INFO SecurityManager: Changing view acls to: root
23/12/31 00:24:54 INFO SecurityManager: Changing modify acls to: root
23/12/31 00:24:54 INFO SecurityManager: Changing view acls groups to: 
23/12/31 00:24:54 INFO SecurityManager: Changing modify acls groups to: 
23/12/31 00:24:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
23/12/31 00:24:54 INFO Utils: Successfully started service 'sparkDriver' on port 43513.
23/12/31 00:24:54 INFO SparkEnv: Registering MapOutputTracker
23/12/31 00:24:54 INFO SparkEnv: Registering BlockManagerMaster
23/12/31 00:24:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/31 00:24:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/31 00:24:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/31 00:24:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-477cebf5-1a9f-447c-b26d-4eccab1427ac
23/12/31 00:24:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
23/12/31 00:24:54 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/31 00:24:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/12/31 00:24:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://eac4a19fd17d:4040
23/12/31 00:24:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.postgresql_postgresql-42.2.10.jar at spark://eac4a19fd17d:43513/jars/org.postgresql_postgresql-42.2.10.jar with timestamp 1703982295059
23/12/31 00:24:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.postgresql_postgresql-42.2.10.jar at spark://eac4a19fd17d:43513/files/org.postgresql_postgresql-42.2.10.jar with timestamp 1703982295060
23/12/31 00:24:55 INFO Utils: Copying /root/.ivy2/jars/org.postgresql_postgresql-42.2.10.jar to /tmp/spark-059825bb-5820-4c2a-938b-d01233731874/userFiles-dfdf9a85-1af4-43d2-8ae2-5f39fb3abfbb/org.postgresql_postgresql-42.2.10.jar
23/12/31 00:24:55 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!
23/12/31 00:24:55 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
23/12/31 00:24:55 INFO TransportClientFactory: Successfully created connection to spark-master/172.30.0.7:7077 after 32 ms (0 ms spent in bootstraps)
23/12/31 00:24:55 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20231231002455-0000
23/12/31 00:24:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36947.
23/12/31 00:24:55 INFO NettyBlockTransferService: Server created on eac4a19fd17d:36947
23/12/31 00:24:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/31 00:24:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, eac4a19fd17d, 36947, None)
23/12/31 00:24:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20231231002455-0000/0 on worker-20231231002438-172.30.0.10-40257 (172.30.0.10:40257) with 8 core(s)
23/12/31 00:24:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20231231002455-0000/0 on hostPort 172.30.0.10:40257 with 8 core(s), 1024.0 MiB RAM
23/12/31 00:24:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20231231002455-0000/1 on worker-20231231002438-172.30.0.11-42917 (172.30.0.11:42917) with 8 core(s)
23/12/31 00:24:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20231231002455-0000/1 on hostPort 172.30.0.11:42917 with 8 core(s), 1024.0 MiB RAM
23/12/31 00:24:55 INFO BlockManagerMasterEndpoint: Registering block manager eac4a19fd17d:36947 with 366.3 MiB RAM, BlockManagerId(driver, eac4a19fd17d, 36947, None)
23/12/31 00:24:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, eac4a19fd17d, 36947, None)
23/12/31 00:24:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, eac4a19fd17d, 36947, None)
23/12/31 00:24:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20231231002455-0000/1 is now RUNNING
23/12/31 00:24:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20231231002455-0000/0 is now RUNNING
23/12/31 00:24:55 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
23/12/31 00:24:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/hive/warehouse').
23/12/31 00:24:56 INFO SharedState: Warehouse path is '/hive/warehouse'.
23/12/31 00:24:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/31 00:24:57 INFO InMemoryFileIndex: It took 94 ms to list leaf files for 1 paths.
23/12/31 00:24:57 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
23/12/31 00:24:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.30.0.11:46754) with ID 1
23/12/31 00:24:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.30.0.10:44594) with ID 0
23/12/31 00:24:58 INFO BlockManagerMasterEndpoint: Registering block manager 172.30.0.11:33461 with 366.3 MiB RAM, BlockManagerId(1, 172.30.0.11, 33461, None)
23/12/31 00:24:58 INFO BlockManagerMasterEndpoint: Registering block manager 172.30.0.10:37975 with 366.3 MiB RAM, BlockManagerId(0, 172.30.0.10, 37975, None)
23/12/31 00:24:59 INFO FileSourceStrategy: Pruning directories with: 
23/12/31 00:24:59 INFO FileSourceStrategy: Pushed Filters: 
23/12/31 00:24:59 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
23/12/31 00:24:59 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/12/31 00:25:00 INFO CodeGenerator: Code generated in 231.187936 ms
23/12/31 00:25:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 306.5 KiB, free 366.0 MiB)
23/12/31 00:25:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 366.0 MiB)
23/12/31 00:25:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on eac4a19fd17d:36947 (size: 27.6 KiB, free: 366.3 MiB)
23/12/31 00:25:00 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
23/12/31 00:25:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5374588 bytes, open cost is considered as scanning 4194304 bytes.
23/12/31 00:25:01 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
23/12/31 00:25:01 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/12/31 00:25:01 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
23/12/31 00:25:01 INFO DAGScheduler: Parents of final stage: List()
23/12/31 00:25:01 INFO DAGScheduler: Missing parents: List()
23/12/31 00:25:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
23/12/31 00:25:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 366.0 MiB)
23/12/31 00:25:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 366.0 MiB)
23/12/31 00:25:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on eac4a19fd17d:36947 (size: 5.3 KiB, free: 366.3 MiB)
23/12/31 00:25:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
23/12/31 00:25:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/12/31 00:25:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
23/12/31 00:25:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.30.0.11, executor 1, partition 0, ANY, 7733 bytes)
23/12/31 00:25:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.30.0.11:33461 (size: 5.3 KiB, free: 366.3 MiB)
23/12/31 00:25:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.30.0.11:33461 (size: 27.6 KiB, free: 366.3 MiB)
23/12/31 00:25:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1604 ms on 172.30.0.11 (executor 1) (1/1)
23/12/31 00:25:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/31 00:25:02 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.683 s
23/12/31 00:25:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/31 00:25:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/31 00:25:02 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.715418 s
23/12/31 00:25:02 INFO CodeGenerator: Code generated in 7.586462 ms
23/12/31 00:25:02 INFO FileSourceStrategy: Pruning directories with: 
23/12/31 00:25:02 INFO FileSourceStrategy: Pushed Filters: 
23/12/31 00:25:02 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/31 00:25:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/12/31 00:25:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 306.5 KiB, free 365.7 MiB)
23/12/31 00:25:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 365.6 MiB)
23/12/31 00:25:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on eac4a19fd17d:36947 (size: 27.6 KiB, free: 366.2 MiB)
23/12/31 00:25:02 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
23/12/31 00:25:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5374588 bytes, open cost is considered as scanning 4194304 bytes.
23/12/31 00:25:03 INFO HiveConf: Found configuration file null
23/12/31 00:25:03 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
23/12/31 00:25:03 INFO HiveConf: Found configuration file null
23/12/31 00:25:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on eac4a19fd17d:36947 in memory (size: 5.3 KiB, free: 366.2 MiB)
23/12/31 00:25:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.30.0.11:33461 in memory (size: 5.3 KiB, free: 366.3 MiB)
23/12/31 00:25:03 INFO SessionState: Created HDFS directory: /tmp/hive/root
23/12/31 00:25:03 INFO SessionState: Created local directory: /tmp/root
23/12/31 00:25:03 INFO SessionState: Created HDFS directory: /tmp/hive/root/aa24a329-c39e-4a1d-9d20-0b45bd3f8fd1
23/12/31 00:25:03 INFO SessionState: Created local directory: /tmp/root/aa24a329-c39e-4a1d-9d20-0b45bd3f8fd1
23/12/31 00:25:03 INFO SessionState: Created HDFS directory: /tmp/hive/root/aa24a329-c39e-4a1d-9d20-0b45bd3f8fd1/_tmp_space.db
23/12/31 00:25:03 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is /hive/warehouse
23/12/31 00:25:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
23/12/31 00:25:03 INFO metastore: Opened a connection to metastore, current connections: 1
23/12/31 00:25:03 INFO metastore: Connected to metastore.
23/12/31 00:25:04 INFO FileSourceStrategy: Pruning directories with: 
23/12/31 00:25:04 INFO FileSourceStrategy: Pushed Filters: 
23/12/31 00:25:04 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/31 00:25:04 INFO FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 1 more field>
23/12/31 00:25:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/31 00:25:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
23/12/31 00:25:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/31 00:25:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/31 00:25:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
23/12/31 00:25:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/31 00:25:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/31 00:25:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 306.5 KiB, free 365.3 MiB)
23/12/31 00:25:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 365.3 MiB)
23/12/31 00:25:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on eac4a19fd17d:36947 (size: 27.6 KiB, free: 366.2 MiB)
23/12/31 00:25:04 INFO SparkContext: Created broadcast 3 from saveAsTable at NativeMethodAccessorImpl.java:0
23/12/31 00:25:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5374588 bytes, open cost is considered as scanning 4194304 bytes.
23/12/31 00:25:04 INFO SparkContext: Starting job: saveAsTable at NativeMethodAccessorImpl.java:0
23/12/31 00:25:04 INFO DAGScheduler: Got job 1 (saveAsTable at NativeMethodAccessorImpl.java:0) with 16 output partitions
23/12/31 00:25:04 INFO DAGScheduler: Final stage: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0)
23/12/31 00:25:04 INFO DAGScheduler: Parents of final stage: List()
23/12/31 00:25:04 INFO DAGScheduler: Missing parents: List()
23/12/31 00:25:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
23/12/31 00:25:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 173.4 KiB, free 365.2 MiB)
23/12/31 00:25:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 61.6 KiB, free 365.1 MiB)
23/12/31 00:25:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on eac4a19fd17d:36947 (size: 61.6 KiB, free: 366.2 MiB)
23/12/31 00:25:04 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
23/12/31 00:25:04 INFO DAGScheduler: Submitting 16 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/12/31 00:25:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 16 tasks
23/12/31 00:25:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 172.30.0.11, executor 1, partition 0, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, 172.30.0.10, executor 0, partition 1, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, 172.30.0.11, executor 1, partition 2, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, 172.30.0.10, executor 0, partition 3, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, 172.30.0.11, executor 1, partition 4, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, 172.30.0.10, executor 0, partition 5, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, 172.30.0.11, executor 1, partition 6, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, 172.30.0.10, executor 0, partition 7, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, 172.30.0.11, executor 1, partition 8, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, 172.30.0.10, executor 0, partition 9, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, 172.30.0.11, executor 1, partition 10, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, 172.30.0.10, executor 0, partition 11, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13, 172.30.0.11, executor 1, partition 12, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14, 172.30.0.10, executor 0, partition 13, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15, 172.30.0.11, executor 1, partition 14, ANY, 7733 bytes)
23/12/31 00:25:04 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16, 172.30.0.10, executor 0, partition 15, ANY, 7733 bytes)
23/12/31 00:25:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.30.0.11:33461 (size: 61.6 KiB, free: 366.2 MiB)
23/12/31 00:25:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.30.0.11:33461 (size: 27.6 KiB, free: 366.2 MiB)
23/12/31 00:25:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.30.0.10:37975 (size: 61.6 KiB, free: 366.2 MiB)
23/12/31 00:25:07 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 3008 ms on 172.30.0.11 (executor 1) (1/16)
23/12/31 00:25:07 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 3010 ms on 172.30.0.11 (executor 1) (2/16)
23/12/31 00:25:07 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 3014 ms on 172.30.0.11 (executor 1) (3/16)
23/12/31 00:25:07 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 3014 ms on 172.30.0.11 (executor 1) (4/16)
23/12/31 00:25:07 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 3016 ms on 172.30.0.11 (executor 1) (5/16)
23/12/31 00:25:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3017 ms on 172.30.0.11 (executor 1) (6/16)
23/12/31 00:25:07 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 3017 ms on 172.30.0.11 (executor 1) (7/16)
23/12/31 00:25:07 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 3014 ms on 172.30.0.11 (executor 1) (8/16)
23/12/31 00:25:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.30.0.10:37975 (size: 27.6 KiB, free: 366.2 MiB)
23/12/31 00:25:09 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 5269 ms on 172.30.0.10 (executor 0) (9/16)
23/12/31 00:25:09 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 5266 ms on 172.30.0.10 (executor 0) (10/16)
23/12/31 00:25:09 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 5271 ms on 172.30.0.10 (executor 0) (11/16)
23/12/31 00:25:09 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 5279 ms on 172.30.0.10 (executor 0) (12/16)
23/12/31 00:25:09 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 5280 ms on 172.30.0.10 (executor 0) (13/16)
23/12/31 00:25:09 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 5279 ms on 172.30.0.10 (executor 0) (14/16)
23/12/31 00:25:09 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 5279 ms on 172.30.0.10 (executor 0) (15/16)
23/12/31 00:25:09 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 5293 ms on 172.30.0.10 (executor 0) (16/16)
23/12/31 00:25:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/31 00:25:09 INFO DAGScheduler: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 5.323 s
23/12/31 00:25:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/31 00:25:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/12/31 00:25:09 INFO DAGScheduler: Job 1 finished: saveAsTable at NativeMethodAccessorImpl.java:0, took 5.327012 s
23/12/31 00:25:09 INFO FileFormatWriter: Write Job c864e557-6db4-4ebd-9172-494c56da75a3 committed.
23/12/31 00:25:09 INFO FileFormatWriter: Finished processing stats for write job c864e557-6db4-4ebd-9172-494c56da75a3.
23/12/31 00:25:09 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.
23/12/31 00:25:09 INFO HiveExternalCatalog: Persisting file based data source table `default`.`test` into Hive metastore in Hive compatible format.
23/12/31 00:25:09 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=aa24a329-c39e-4a1d-9d20-0b45bd3f8fd1, clientType=HIVECLI]
23/12/31 00:25:09 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
23/12/31 00:25:09 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
23/12/31 00:25:09 INFO metastore: Closed a connection to metastore, current connections: 0
23/12/31 00:25:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
23/12/31 00:25:09 INFO metastore: Opened a connection to metastore, current connections: 1
23/12/31 00:25:09 INFO metastore: Connected to metastore.
23/12/31 00:25:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
23/12/31 00:25:09 INFO metastore: Opened a connection to metastore, current connections: 2
23/12/31 00:25:09 INFO metastore: Connected to metastore.
23/12/31 00:25:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on eac4a19fd17d:36947 in memory (size: 61.6 KiB, free: 366.2 MiB)
23/12/31 00:25:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.30.0.11:33461 in memory (size: 61.6 KiB, free: 366.2 MiB)
23/12/31 00:25:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.30.0.10:37975 in memory (size: 61.6 KiB, free: 366.3 MiB)
23/12/31 00:25:09 INFO SparkContext: Invoking stop() from shutdown hook
23/12/31 00:25:09 INFO SparkUI: Stopped Spark web UI at http://eac4a19fd17d:4040
23/12/31 00:25:09 INFO StandaloneSchedulerBackend: Shutting down all executors
23/12/31 00:25:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
23/12/31 00:25:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/31 00:25:10 INFO MemoryStore: MemoryStore cleared
23/12/31 00:25:10 INFO BlockManager: BlockManager stopped
23/12/31 00:25:10 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/31 00:25:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/31 00:25:10 INFO SparkContext: Successfully stopped SparkContext
23/12/31 00:25:10 INFO ShutdownHookManager: Shutdown hook called
23/12/31 00:25:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-059825bb-5820-4c2a-938b-d01233731874/pyspark-b0f95cb1-a323-4872-b0e8-d0e0dbc935a0
23/12/31 00:25:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee1b56be-5af9-4725-810f-dfca5f0002b6
23/12/31 00:25:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-059825bb-5820-4c2a-938b-d01233731874
